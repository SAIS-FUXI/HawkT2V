# dataset
dataset: "t2v"
video_folder: ''

dataset_config: {
  'fps': 24,
  'seconds': 2,
  'frames_per_second': 8
}


video_json: "samples/video_data.json"
image_json: "samples/image_data.json"
neg_text_embedding_path:  "pretrained_models/text_encoder/null_embedd.pkl"


# pretrained models
pretrained_model_path: False

# pretrained t2v checkpint file:
pretrained: ""

pretrained_text_model_path: "pretrained_models/text_encoder/"
text_encoder_cpu: False

vae_config: "pretrained_models/vae/config.yaml"
vae_model_path: "pretrained_models/vae/diffusion_pytorch_model.bin"
decode_length: 5
vae_scale_factor: 0.22970
vae_cpu: True

# where to save results:
results_dir: "./results/video_vae_hawkt2v_batch_video"

# model config: 
model: HawkT2V
HawkT2V: {
  "activation_fn": "gelu-approximate",
  "attention_bias": true,
  "attention_head_dim": 72, # width change 72->72/80
  "attention_type": "default",
  "caption_channels": 4096,
  "cross_attention_dim": 1728, # width change 1152 -> 1728/1600
  "double_self_attention": false,
  "dropout": 0.0,
  "in_channels": 8,
  "norm_elementwise_affine": false,
  "norm_eps": 1e-06,
  "norm_num_groups": 32,
  "norm_type": "ada_norm_single",
  "num_attention_heads": 24, # width change 16->24/20
  "num_embeds_ada_norm": 1000,
  "num_layers": 32, # depth change 28 -> 32
  "num_vector_embeds": null,
  "only_cross_attention": false,
  "patch_size": 2,
  "sample_size": 64,
  "upcast_attention": false,
  "use_linear_projection": false,
  "video_length": 5,
  "learn_sigma": True,
  "attention_mixed": '',
  "window_size": [-1, 32, 32],
  "anchor_video_length": 5,
  "use_additional_conditions": False
}


use_fsdp: True
video_length: 5
num_frames: 16
image_size: 512 # choices=[256, 512]
num_sampling_steps: 250
frame_interval: 1
fixed_spatial: False
attention_bias: True
learn_sigma: True # important
extras: 78 # [1, 2, 78]


# train config:
save_ceph: True # important
use_image_num: 2
use_video_num: 1
learning_rate: 1e-4
weight_decay: 0.0
lr_warmup_steps: 1000
scheduler_type: 'constant_with_warmup'
ckpt_every: 1000
clip_max_norm: 0.1
start_clip_iter: 1000
local_batch_size: 1 # important
max_train_steps: 1000000 #1000000
global_seed: 3407
num_workers: 4
log_every: 1
resume_from_checkpoint:
gradient_accumulation_steps: 1
num_classes:
use_wandb: False
validation_before_training: True
cfg_random_null_text_ratio: 0.1

# low VRAM and speed up training
use_compile: False
mixed_precision: False
enable_xformers_memory_efficient_attention: True
gradient_checkpointing: True


# validation gen:
validation:

  # model config:
  video_length: 5
  image_size: [512, 512]
  # # beta scheduleextras
  beta_start: 0.0001
  beta_end: 0.02
  beta_schedule: "linear"
  variance_type: "learned_range"

  # model speedup
  use_compile: False
  use_fp16: True

  # sample config:
  seed:
  run_time: 0
  guidance_scale: 15
  sample_method: 'PNDM'
  num_sampling_steps: 50
  enable_temporal_attentions: True
  enable_vae_temporal_decoder: False # use temporal vae decoder from SVD, maybe reduce the video flicker (It's not widely tested)

  # add your own testing prompts to validate the training process
  text_prompt: [
    'Yellow and black tropical fish dart through the sea.',
    'An epic tornado attacking above aglowing city at night.',
    'Slow pan upward of blazing oak fire in an indoor fireplace.',
    "a photo of an astronaut riding a horse on mars",
  ]